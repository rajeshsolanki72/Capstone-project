---
title: "my-capstone-project-1"
author: "Rajesh Solanki"
date: "October 22, 2018"
output: html_document
---

```{r setup,cache=TRUE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Objective

The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to: 

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.
4. Get feedback on your plans for creating a prediction algorithm and Shiny app. 
```{r, echo=FALSE}
setwd("C:/Users/rajes/iCloudDrive/Data-Science-Specialization/Capstone")
```

## Data acquisition
```{r , warning=FALSE, message=FALSE}
library(SnowballC);library(ngram);library(rJava);library(RWeka);library(ggplot2)
library(tm);library(wordcloud);library(RColorBrewer); library(stringi); library(dplyr)
fileUrl <-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists("Dataset.zip")){
  download.file(fileUrl, destfile = "Dataset.zip")
  unzip("Dataset.zip")
 }
bwsrc1<-"https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
if (!file.exists("en_bws.txt")){download.file(bwsrc1, destfile="en_bws.txt")}


```

## loading Data

```{r,warning=FALSE, message=FALSE}
setwd("C:/Users/rajes/iCloudDrive/Data-Science-Specialization/Capstone/final")
twitter_con <- file('en_US/en_US.twitter.txt', encoding = 'UTF-8')
twitter <- readLines(twitter_con, skipNul = TRUE)
news_con <-file('en_US/en_US.news.txt', encoding = 'UTF-8')
news <- readLines(news_con, skipNul = TRUE)
blogs_con <- file('en_US/en_US.blogs.txt', encoding = 'UTF-8')
blogs <- readLines(blogs_con, skipNul = TRUE)
close(twitter_con)
close(news_con)
close(blogs_con)
```

The data consist of text from 3 different sources: blogs, news, and twitter feeds and are provided in 4 different languages: German, English (US), Finnish, and Russian. For the remainder of this project, we will use only the the English (US) data sets.

## Data Summary

Checking the size and length of the files and calculate the word count

```{r}
setwd("C:/Users/rajes/iCloudDrive/Data-Science-Specialization/Capstone/final")
blogsFile <- file.info("en_US/en_US.blogs.txt")$size / 1024.0 ^2 
newsFile <- file.info("en_US/en_US.news.txt")$size / 1024.0 ^ 2
twitterFile <- file.info("en_US/en_US.twitter.txt")$size / 1024.0 ^ 2

blogsLength <- length(blogs)
newsLength <- length(news)
twitterLength <- length(twitter)

blogsWords <- wordcount(blogs, sep = " ", count.function = sum) 
newsWords <- wordcount(news, sep = " ", count.function = sum)
twitterWords <- wordcount(twitter, sep = " ", count.function = sum)

fileSummary <- data.frame(
  fileName = c("Blogs","News","Twitter"),
  fileSize = c(round(blogsFile, digits = 2), 
               round(newsFile,digits = 2), 
               round(twitterFile, digits = 2)),
  lineCount = c(blogsLength, newsLength, twitterLength),
  wordCount = c(blogsWords, newsWords, twitterWords)                  
) 

colnames(fileSummary)<- c("File Name", "File Size in Megabyte", "Line Count", "Word Count")
print(fileSummary)
```

These datasets are rather large, and since the goal is to provide a proof of concept for the data analysis, for the remainder of the report we will sample a smaller fraction of the data (we will use first 1000 lines ) to perform the analysis. The three parts will be combine into a single file and used to generate the corpus.

## Generating a random sapmle of all sources

```{r}
sampleTwitter <- twitter[sample(1:length(twitter),20000)]

sampleNews <- news[sample(1:length(news),20000)]

sampleBlogs <- blogs[sample(1:length(blogs),20000)]

textSample <- c(sampleTwitter,sampleNews,sampleBlogs)
```

## Build the corpus, and specify the source to be character vectors 

```{r}
corpus <- VCorpus(VectorSource(textSample))
```
## Cleaning the text

Make it work with the new tm package to Convert to lower case,remove punction, 
numbers, URLs, stop, profanity and stem wordson
```{r}
profanityWords <- read.csv("en_bws.txt")

corpus <- corpus %>%
  tm_map(tolower) %>%  
  tm_map(PlainTextDocument) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(stemDocument)%>%
  tm_map(removeWords, stopwords())%>%
  tm_map(removeWords,profanityWords$X2g1c)%>%
  tm_map(stripWhitespace)  
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x) 
saveRDS(corpus, file = "finalCorpus.RDS")
finalCorpus <- readRDS("finalCorpus.RDS")
finalCorpusDF <-data.frame(text=unlist(sapply(finalCorpus,`[`, "content")), 
                          stringsAsFactors = FALSE)
```

## Exploratory analysis 

```{r}
term.doc.matrix <- TermDocumentMatrix(finalCorpus)
term.doc.matrix <- removeSparseTerms(term.doc.matrix, 0.999)
term.doc.matrix <- as.matrix(term.doc.matrix)
word.freqs <- sort(rowSums(term.doc.matrix), decreasing=TRUE) 
dm <- data.frame(word=names(word.freqs), freq=word.freqs)
```

## Word cloud plot of the most common words in the corpus
```{r , warning= FALSE}
wordcloud(dm$word, dm$freq, min.freq= 500, random.order=TRUE, rot.per=.25, 
          colors=brewer.pal(8, "Dark2"))
```

## Building the tokenization function for the n-grams
```{r}
ngramTokenizer <- function(theCorpus, ngramCount) {
  
  ngramFunction <- NGramTokenizer(theCorpus, 
                   Weka_control(min = ngramCount,
                   max = ngramCount,delimiters = " \\r\\n\\t.,;:\"()?!"))
  ngramFunction <- data.frame(table(ngramFunction))
  ngramFunction <- ngramFunction[order(ngramFunction$Freq, 
                   decreasing = TRUE),][1:10,]
  colnames(ngramFunction) <- c("String","Count")
  ngramFunction
}
unigram <- ngramTokenizer(finalCorpusDF, 1)
bigram <- ngramTokenizer(finalCorpusDF, 2)
trigram <- ngramTokenizer(finalCorpusDF, 3)
```

## unigram frequency distribution plot
```{r}
ggplot(unigram[1:25,], aes(x=String, y=Count)) +
  geom_bar(stat="Identity", fill="#0047AB")+
  xlab("Unigrams") + ylab("Frequency")+
  ggtitle("Most common 25 Unigrams") + theme_bw() + theme(plot.title = element_text(hjust=0.5)) +
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

## bigram frequency distribution plot
```{r}
ggplot(bigram[1:25,], aes(x=String, y=Count)) +
  geom_bar(stat="Identity", fill="#0047AB")+
  xlab("Unigrams") + ylab("Frequency")+
  ggtitle("Most common 25 bigrams") + theme_bw() + theme(plot.title = element_text(hjust=0.5)) +
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

## trigram frequency distribution plot
```{r}
ggplot(trigram[1:25,], aes(x=String, y=Count)) +
  geom_bar(stat="Identity", fill="#0047AB")+
  xlab("Unigrams") + ylab("Frequency")+
  ggtitle("Most common 25 trigrams") +theme_bw() + theme(plot.title = element_text(hjust=0.5)) +
  theme(axis.text.x=element_text(angle=90, hjust=1))
```


## Future Prediction algorithm and app idea 

1- In future I will be developing a prediction model for recomanding next possible word.

2- We will use N-gram model to predict cluster of possible words. 




